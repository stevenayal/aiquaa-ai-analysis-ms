"""
Microservicio de An치lisis QA con Langfuse
FastAPI Service para an치lisis automatizado de casos de prueba
"""

import os
import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime
import structlog
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field
from dotenv import load_dotenv

# Importar m칩dulos locales
from tracker_client import TrackerClient
from llm_wrapper import LLMWrapper
from prompt_templates import PromptTemplates
from sanitizer import PIISanitizer

# Cargar variables de entorno
load_dotenv()

# Configurar logging estructurado
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()

# Inicializar FastAPI
app = FastAPI(
    title="Microservicio de An치lisis QA",
    description="""
    ## API de An치lisis Automatizado de Casos de Prueba
    
    Esta API proporciona an치lisis inteligente de casos de prueba utilizando IA generativa.
    
    ### Caracter칤sticas:
    - 游뱄 An치lisis automatizado con Google Gemini
    - 游늵 Observabilidad completa con Langfuse
    - 游댕 Integraci칩n con Jira
    - 游닇 Sugerencias de mejora estructuradas
    - 游 Procesamiento en lote
    
    ### Autenticaci칩n:
    No se requiere autenticaci칩n para las pruebas locales.
    
    ### Uso:
    1. Env칤a un caso de prueba al endpoint `/analyze`
    2. Recibe sugerencias de mejora estructuradas
    3. Usa `/batch-analyze` para m칰ltiples casos
    4. Monitorea el estado con `/health`
    """,
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_url="/openapi.json",
    contact={
        "name": "Equipo de QA",
        "email": "qa-team@company.com",
    },
    license_info={
        "name": "MIT",
    },
    servers=[
        {
            "url": "http://localhost:8000",
            "description": "Servidor de desarrollo local"
        }
    ]
)

# Inicializar componentes
tracker_client = TrackerClient()
llm_wrapper = LLMWrapper()
prompt_templates = PromptTemplates()
sanitizer = PIISanitizer()

# Modelos Pydantic
class TestCaseAnalysisRequest(BaseModel):
    """Solicitud de an치lisis de caso de prueba"""
    test_case_id: str = Field(
        ..., 
        description="ID 칰nico del caso de prueba",
        example="TC-001",
        min_length=1,
        max_length=50
    )
    test_case_content: str = Field(
        ..., 
        description="Descripci칩n detallada del caso de prueba a analizar",
        example="Verificar que el usuario pueda iniciar sesi칩n con credenciales v치lidas",
        min_length=10,
        max_length=5000
    )
    project_key: str = Field(
        ..., 
        description="Clave del proyecto en Jira",
        example="TEST",
        min_length=1,
        max_length=20
    )
    priority: Optional[str] = Field(
        "Medium", 
        description="Prioridad del caso de prueba",
        example="High",
        pattern="^(Low|Medium|High|Critical)$"
    )
    labels: Optional[List[str]] = Field(
        default_factory=list, 
        description="Etiquetas para categorizar el caso de prueba",
        example=["login", "authentication", "smoke-test"]
    )
    
    class Config:
        schema_extra = {
            "example": {
                "test_case_id": "TC-001",
                "test_case_content": "Verificar que el usuario pueda iniciar sesi칩n con credenciales v치lidas. Pasos: 1) Abrir la p치gina de login, 2) Ingresar usuario v치lido, 3) Ingresar contrase침a v치lida, 4) Hacer clic en 'Iniciar Sesi칩n'. Resultado esperado: Usuario logueado exitosamente y redirigido al dashboard.",
                "project_key": "TEST",
                "priority": "High",
                "labels": ["login", "authentication", "smoke-test"]
            }
        }

class Suggestion(BaseModel):
    """Sugerencia de mejora para un caso de prueba"""
    type: str = Field(..., description="Tipo de sugerencia", example="clarity")
    title: str = Field(..., description="T칤tulo de la sugerencia", example="Definir datos de prueba espec칤ficos")
    description: str = Field(..., description="Descripci칩n detallada", example="El caso de prueba debe incluir datos espec칤ficos de usuario y contrase침a")
    priority: str = Field(..., description="Prioridad de la sugerencia", example="high")
    category: str = Field(..., description="Categor칤a de la mejora", example="improvement")

class TestCaseAnalysisResponse(BaseModel):
    """Respuesta del an치lisis de caso de prueba"""
    test_case_id: str = Field(..., description="ID del caso de prueba analizado", example="TC-001")
    analysis_id: str = Field(..., description="ID 칰nico del an치lisis", example="analysis_TC001_1760825804")
    status: str = Field(..., description="Estado del an치lisis", example="completed")
    suggestions: List[Suggestion] = Field(..., description="Lista de sugerencias de mejora")
    confidence_score: float = Field(..., description="Puntuaci칩n de confianza del an치lisis (0-1)", example=0.85)
    processing_time: float = Field(..., description="Tiempo de procesamiento en segundos", example=8.81)
    created_at: datetime = Field(..., description="Timestamp de creaci칩n del an치lisis")
    
    class Config:
        schema_extra = {
            "example": {
                "test_case_id": "TC-001",
                "analysis_id": "analysis_TC001_1760825804",
                "status": "completed",
                "suggestions": [
                    {
                        "type": "clarity",
                        "title": "Definir datos de prueba espec칤ficos",
                        "description": "El caso de prueba debe incluir datos espec칤ficos de usuario y contrase침a",
                        "priority": "high",
                        "category": "improvement"
                    }
                ],
                "confidence_score": 0.85,
                "processing_time": 8.81,
                "created_at": "2025-10-18T19:16:44.520862"
            }
        }

class RequirementsAnalysisRequest(BaseModel):
    """Solicitud de an치lisis de requerimientos para generar casos de prueba"""
    requirement_id: str = Field(
        ..., 
        description="ID 칰nico del requerimiento",
        example="REQ-001",
        min_length=1,
        max_length=50
    )
    requirement_content: str = Field(
        ..., 
        description="Descripci칩n detallada del requerimiento a analizar",
        example="El sistema debe permitir a los usuarios autenticarse usando email y contrase침a",
        min_length=10,
        max_length=10000
    )
    project_key: str = Field(
        ..., 
        description="Clave del proyecto en Jira",
        example="AUTH",
        min_length=1,
        max_length=20
    )
    priority: Optional[str] = Field(
        "Medium", 
        description="Prioridad del requerimiento",
        example="High",
        pattern="^(Low|Medium|High|Critical)$"
    )
    test_types: Optional[List[str]] = Field(
        default_factory=lambda: ["functional", "integration"],
        description="Tipos de pruebas a generar",
        example=["functional", "integration", "ui", "api", "security"]
    )
    coverage_level: Optional[str] = Field(
        "medium",
        description="Nivel de cobertura de pruebas",
        example="high",
        pattern="^(low|medium|high|comprehensive)$"
    )
    
    class Config:
        json_schema_extra = {
            "example": {
                "requirement_id": "REQ-001",
                "requirement_content": "El sistema debe permitir a los usuarios autenticarse usando email y contrase침a. El sistema debe validar las credenciales contra la base de datos y permitir el acceso solo a usuarios activos. En caso de credenciales incorrectas, debe mostrar un mensaje de error apropiado.",
                "project_key": "AUTH",
                "priority": "High",
                "test_types": ["functional", "integration", "security"],
                "coverage_level": "high"
            }
        }

class TestCase(BaseModel):
    """Caso de prueba generado"""
    test_case_id: str = Field(..., description="ID del caso de prueba", example="TC-AUTH-001")
    title: str = Field(..., description="T칤tulo del caso de prueba", example="Verificar login con credenciales v치lidas")
    description: str = Field(..., description="Descripci칩n detallada del caso de prueba")
    test_type: str = Field(..., description="Tipo de prueba", example="functional")
    priority: str = Field(..., description="Prioridad del caso de prueba", example="high")
    steps: List[str] = Field(..., description="Pasos detallados del caso de prueba")
    expected_result: str = Field(..., description="Resultado esperado")
    preconditions: List[str] = Field(default_factory=list, description="Precondiciones necesarias")
    test_data: Optional[Dict[str, Any]] = Field(default_factory=dict, description="Datos de prueba espec칤ficos")
    automation_potential: str = Field(..., description="Potencial de automatizaci칩n", example="high")
    estimated_duration: str = Field(..., description="Duraci칩n estimada", example="5-10 minutes")

class RequirementsAnalysisResponse(BaseModel):
    """Respuesta del an치lisis de requerimientos"""
    requirement_id: str = Field(..., description="ID del requerimiento analizado", example="REQ-001")
    analysis_id: str = Field(..., description="ID 칰nico del an치lisis", example="req_analysis_REQ001_1760825804")
    status: str = Field(..., description="Estado del an치lisis", example="completed")
    test_cases: List[TestCase] = Field(..., description="Lista de casos de prueba generados")
    coverage_analysis: Dict[str, Any] = Field(..., description="An치lisis de cobertura de pruebas")
    confidence_score: float = Field(..., description="Puntuaci칩n de confianza del an치lisis (0-1)", example=0.85)
    processing_time: float = Field(..., description="Tiempo de procesamiento en segundos", example=12.5)
    created_at: datetime = Field(..., description="Timestamp de creaci칩n del an치lisis")
    
    class Config:
        json_schema_extra = {
            "example": {
                "requirement_id": "REQ-001",
                "analysis_id": "req_analysis_REQ001_1760825804",
                "status": "completed",
                "test_cases": [
                    {
                        "test_case_id": "TC-AUTH-001",
                        "title": "Verificar login con credenciales v치lidas",
                        "description": "Caso de prueba para verificar que un usuario puede autenticarse exitosamente",
                        "test_type": "functional",
                        "priority": "high",
                        "steps": [
                            "Navegar a la p치gina de login",
                            "Ingresar email v치lido",
                            "Ingresar contrase침a v치lida",
                            "Hacer clic en 'Iniciar Sesi칩n'"
                        ],
                        "expected_result": "Usuario autenticado exitosamente y redirigido al dashboard",
                        "preconditions": ["Usuario existe en la base de datos", "Usuario est치 activo"],
                        "test_data": {"email": "test@example.com", "password": "Test123!"},
                        "automation_potential": "high",
                        "estimated_duration": "5-10 minutes"
                    }
                ],
                "coverage_analysis": {
                    "functional_coverage": "90%",
                    "edge_case_coverage": "75%",
                    "integration_coverage": "80%"
                },
                "confidence_score": 0.85,
                "processing_time": 12.5,
                "created_at": "2025-10-18T19:16:44.520862"
            }
        }

class JiraWorkItemRequest(BaseModel):
    """Solicitud de an치lisis de work item de Jira"""
    work_item_id: str = Field(
        ..., 
        description="ID del work item en Jira (ej: PROJ-123)",
        example="AUTH-123",
        min_length=1,
        max_length=50
    )
    project_key: str = Field(
        ..., 
        description="Clave del proyecto en Jira",
        example="AUTH",
        min_length=1,
        max_length=20
    )
    test_types: Optional[List[str]] = Field(
        default_factory=lambda: ["functional", "integration"],
        description="Tipos de pruebas a generar",
        example=["functional", "integration", "ui", "api", "security"]
    )
    coverage_level: Optional[str] = Field(
        "medium",
        description="Nivel de cobertura de pruebas",
        example="high",
        pattern="^(low|medium|high|comprehensive)$"
    )
    include_acceptance_criteria: Optional[bool] = Field(
        True,
        description="Incluir criterios de aceptaci칩n en el an치lisis",
        example=True
    )
    
    class Config:
        json_schema_extra = {
            "example": {
                "work_item_id": "AUTH-123",
                "project_key": "AUTH",
                "test_types": ["functional", "integration", "security"],
                "coverage_level": "high",
                "include_acceptance_criteria": True
            }
        }

class JiraWorkItemResponse(BaseModel):
    """Respuesta del an치lisis de work item de Jira"""
    work_item_id: str = Field(..., description="ID del work item analizado", example="AUTH-123")
    jira_data: Dict[str, Any] = Field(..., description="Datos obtenidos de Jira")
    analysis_id: str = Field(..., description="ID 칰nico del an치lisis", example="jira_analysis_AUTH123_1760825804")
    status: str = Field(..., description="Estado del an치lisis", example="completed")
    test_cases: List[TestCase] = Field(..., description="Lista de casos de prueba generados")
    coverage_analysis: Dict[str, Any] = Field(..., description="An치lisis de cobertura de pruebas")
    confidence_score: float = Field(..., description="Puntuaci칩n de confianza del an치lisis (0-1)", example=0.85)
    processing_time: float = Field(..., description="Tiempo de procesamiento en segundos", example=15.5)
    created_at: datetime = Field(..., description="Timestamp de creaci칩n del an치lisis")
    
    class Config:
        json_schema_extra = {
            "example": {
                "work_item_id": "AUTH-123",
                "jira_data": {
                    "summary": "Implementar autenticaci칩n de usuarios",
                    "description": "El sistema debe permitir a los usuarios autenticarse...",
                    "issue_type": "Story",
                    "priority": "High",
                    "status": "In Progress"
                },
                "analysis_id": "jira_analysis_AUTH123_1760825804",
                "status": "completed",
                "test_cases": [
                    {
                        "test_case_id": "TC-AUTH-001",
                        "title": "Verificar login con credenciales v치lidas",
                        "description": "Caso de prueba para verificar autenticaci칩n exitosa",
                        "test_type": "functional",
                        "priority": "high",
                        "steps": ["Navegar a login", "Ingresar credenciales", "Hacer clic en login"],
                        "expected_result": "Usuario autenticado exitosamente",
                        "preconditions": ["Usuario existe en BD"],
                        "test_data": {"email": "test@example.com", "password": "Test123!"},
                        "automation_potential": "high",
                        "estimated_duration": "5-10 minutes"
                    }
                ],
                "coverage_analysis": {
                    "functional_coverage": "90%",
                    "edge_case_coverage": "75%",
                    "integration_coverage": "80%"
                },
                "confidence_score": 0.85,
                "processing_time": 15.5,
                "created_at": "2025-10-18T19:16:44.520862"
            }
        }

class HealthResponse(BaseModel):
    """Respuesta de salud del servicio"""
    status: str
    timestamp: datetime
    version: str
    components: Dict[str, str]

@app.get("/", 
         response_model=Dict[str, str],
         summary="Informaci칩n del servicio",
         description="Endpoint ra칤z que proporciona informaci칩n b치sica sobre el microservicio de an치lisis QA",
         tags=["Informaci칩n"])
async def root():
    """
    ## Informaci칩n del Servicio
    
    Retorna informaci칩n b치sica sobre el microservicio de an치lisis QA.
    
    ### Respuesta:
    - **message**: Descripci칩n del servicio
    - **version**: Versi칩n actual de la API
    - **docs**: URL de la documentaci칩n Swagger
    """
    return {
        "message": "Microservicio de An치lisis QA",
        "version": "1.0.0",
        "docs": "/docs"
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Verificaci칩n de salud del servicio"""
    components = {}
    
    # Verificar Langfuse
    try:
        await llm_wrapper.health_check()
        components["langfuse"] = "healthy"
    except Exception as e:
        logger.error("Langfuse health check failed", error=str(e))
        components["langfuse"] = "unhealthy"
    
    # Verificar Jira
    try:
        await tracker_client.health_check()
        components["jira"] = "healthy"
    except Exception as e:
        logger.error("Jira health check failed", error=str(e))
        components["jira"] = "unhealthy"
    
    # Verificar LLM
    try:
        await llm_wrapper.test_connection()
        components["llm"] = "healthy"
    except Exception as e:
        logger.error("LLM health check failed", error=str(e))
        components["llm"] = "unhealthy"
    
    overall_status = "healthy" if all(status == "healthy" for status in components.values()) else "degraded"
    
    return HealthResponse(
        status=overall_status,
        timestamp=datetime.utcnow(),
        version="1.0.0",
        components=components
    )

@app.post("/analyze", 
          response_model=TestCaseAnalysisResponse,
          summary="Analizar caso de prueba",
          description="Analiza un caso de prueba individual y genera sugerencias de mejora usando IA",
          tags=["An치lisis"],
          responses={
              200: {
                  "description": "An치lisis completado exitosamente",
                  "model": TestCaseAnalysisResponse
              },
              400: {
                  "description": "Datos de entrada inv치lidos",
                  "content": {
                      "application/json": {
                          "example": {"detail": "Datos de entrada inv치lidos"}
                      }
                  }
              },
              500: {
                  "description": "Error interno del servidor",
                  "content": {
                      "application/json": {
                          "example": {"detail": "Error interno del servidor"}
                      }
                  }
              }
          })
async def analyze_test_case(
    request: TestCaseAnalysisRequest,
    background_tasks: BackgroundTasks
):
    """
    ## Analizar Caso de Prueba
    
    Analiza un caso de prueba individual utilizando IA generativa (Google Gemini) y genera sugerencias de mejora estructuradas.
    
    ### Proceso:
    1. **Sanitizaci칩n**: Se elimina informaci칩n sensible del contenido
    2. **An치lisis IA**: Se procesa con Google Gemini usando prompts especializados
    3. **Estructuraci칩n**: Se organizan las sugerencias en categor칤as
    4. **Observabilidad**: Se registra en Langfuse para monitoreo
    
    ### Tipos de Sugerencias:
    - **Clarity**: Mejoras en claridad y legibilidad
    - **Coverage**: Sugerencias para mejorar cobertura de pruebas
    - **Automation**: Optimizaciones para automatizaci칩n
    - **Best Practice**: Mejores pr치cticas de testing
    
    ### Respuesta:
    - **suggestions**: Lista de sugerencias categorizadas
    - **confidence_score**: Puntuaci칩n de confianza (0-1)
    - **processing_time**: Tiempo de procesamiento en segundos
    """
    start_time = datetime.utcnow()
    analysis_id = f"analysis_{request.test_case_id}_{int(start_time.timestamp())}"
    
    try:
        logger.info(
            "Starting test case analysis",
            test_case_id=request.test_case_id,
            analysis_id=analysis_id,
            project_key=request.project_key
        )
        
        # Sanitizar contenido sensible
        sanitized_content = sanitizer.sanitize(request.test_case_content)
        
        # Obtener prompt para an치lisis
        prompt = prompt_templates.get_analysis_prompt(
            test_case_content=sanitized_content,
            project_key=request.project_key,
            priority=request.priority,
            labels=request.labels
        )
        
        # Ejecutar an치lisis con LLM
        analysis_result = await llm_wrapper.analyze_test_case(
            prompt=prompt,
            test_case_id=request.test_case_id,
            analysis_id=analysis_id
        )
        
        # Procesar sugerencias
        suggestions = []
        if analysis_result.get("suggestions"):
            for suggestion in analysis_result["suggestions"]:
                suggestions.append({
                    "type": suggestion.get("type", "general"),
                    "title": suggestion.get("title", ""),
                    "description": suggestion.get("description", ""),
                    "priority": suggestion.get("priority", "medium"),
                    "category": suggestion.get("category", "improvement")
                })
        
        # Calcular tiempo de procesamiento
        processing_time = (datetime.utcnow() - start_time).total_seconds()
        
        # Crear respuesta
        response = TestCaseAnalysisResponse(
            test_case_id=request.test_case_id,
            analysis_id=analysis_id,
            status="completed",
            suggestions=suggestions,
            confidence_score=analysis_result.get("confidence_score", 0.8),
            processing_time=processing_time,
            created_at=start_time
        )
        
        # Registrar en background task para tracking
        background_tasks.add_task(
            log_analysis_completion,
            analysis_id,
            request.test_case_id,
            response
        )
        
        logger.info(
            "Test case analysis completed",
            test_case_id=request.test_case_id,
            analysis_id=analysis_id,
            suggestions_count=len(suggestions),
            processing_time=processing_time
        )
        
        return response
        
    except Exception as e:
        logger.error(
            "Test case analysis failed",
            test_case_id=request.test_case_id,
            analysis_id=analysis_id,
            error=str(e)
        )
        raise HTTPException(
            status_code=500,
            detail=f"Error analyzing test case: {str(e)}"
        )

@app.post("/batch-analyze")
async def batch_analyze_test_cases(
    requests: List[TestCaseAnalysisRequest],
    background_tasks: BackgroundTasks
):
    """
    Analizar m칰ltiples casos de prueba en lote
    """
    logger.info("Starting batch analysis", count=len(requests))
    
    results = []
    for request in requests:
        try:
            result = await analyze_test_case(request, background_tasks)
            results.append(result)
        except Exception as e:
            logger.error(
                "Failed to analyze test case in batch",
                test_case_id=request.test_case_id,
                error=str(e)
            )
            results.append({
                "test_case_id": request.test_case_id,
                "status": "failed",
                "error": str(e)
            })
    
    return {
        "total_processed": len(requests),
        "successful": len([r for r in results if r.get("status") == "completed"]),
        "failed": len([r for r in results if r.get("status") == "failed"]),
        "results": results
    }

@app.post("/analyze-requirements", 
          response_model=RequirementsAnalysisResponse,
          summary="Analizar requerimientos y generar casos de prueba",
          description="Analiza un requerimiento y genera casos de prueba estructurados usando IA",
          tags=["Requerimientos"],
          responses={
              200: {
                  "description": "An치lisis de requerimientos completado exitosamente",
                  "model": RequirementsAnalysisResponse
              },
              400: {
                  "description": "Datos de entrada inv치lidos",
                  "content": {
                      "application/json": {
                          "example": {"detail": "Datos de entrada inv치lidos"}
                      }
                  }
              },
              500: {
                  "description": "Error interno del servidor",
                  "content": {
                      "application/json": {
                          "example": {"detail": "Error interno del servidor"}
                      }
                  }
              }
          })
async def analyze_requirements(
    request: RequirementsAnalysisRequest,
    background_tasks: BackgroundTasks
):
    """
    ## Analizar Requerimientos y Generar Casos de Prueba
    
    Analiza un requerimiento de software y genera casos de prueba estructurados utilizando IA generativa.
    
    ### Proceso:
    1. **An치lisis del Requerimiento**: Se procesa el contenido del requerimiento
    2. **Generaci칩n de Casos**: Se crean casos de prueba para diferentes tipos y niveles
    3. **Estructuraci칩n**: Se organizan los casos con pasos, datos y resultados esperados
    4. **An치lisis de Cobertura**: Se eval칰a la cobertura de pruebas generada
    
    ### Tipos de Pruebas Soportados:
    - **Functional**: Pruebas funcionales b치sicas
    - **Integration**: Pruebas de integraci칩n
    - **UI**: Pruebas de interfaz de usuario
    - **API**: Pruebas de API
    - **Security**: Pruebas de seguridad
    - **Performance**: Pruebas de rendimiento
    
    ### Niveles de Cobertura:
    - **Low**: Casos b치sicos esenciales
    - **Medium**: Casos est치ndar con casos edge
    - **High**: Cobertura completa con casos complejos
    - **Comprehensive**: Cobertura exhaustiva con todos los escenarios
    
    ### Respuesta:
    - **test_cases**: Lista de casos de prueba generados
    - **coverage_analysis**: An치lisis de cobertura por tipo
    - **confidence_score**: Puntuaci칩n de confianza (0-1)
    - **processing_time**: Tiempo de procesamiento en segundos
    """
    start_time = datetime.utcnow()
    analysis_id = f"req_analysis_{request.requirement_id}_{int(start_time.timestamp())}"
    
    try:
        logger.info(
            "Starting requirements analysis",
            requirement_id=request.requirement_id,
            analysis_id=analysis_id,
            project_key=request.project_key
        )
        
        # Sanitizar contenido sensible
        sanitized_content = sanitizer.sanitize(request.requirement_content)
        
        # Generar prompt para an치lisis de requerimientos
        prompt = prompt_templates.get_requirements_analysis_prompt(
            requirement_content=sanitized_content,
            project_key=request.project_key,
            priority=request.priority,
            test_types=request.test_types,
            coverage_level=request.coverage_level
        )
        
        # Ejecutar an치lisis con LLM
        analysis_result = await llm_wrapper.analyze_requirements(
            prompt=prompt,
            requirement_id=request.requirement_id,
            analysis_id=analysis_id
        )
        
        # Procesar casos de prueba generados
        test_cases = []
        if analysis_result.get("test_cases"):
            for tc_data in analysis_result["test_cases"]:
                test_case = TestCase(
                    test_case_id=tc_data.get("test_case_id", f"TC-{request.requirement_id}-001"),
                    title=tc_data.get("title", ""),
                    description=tc_data.get("description", ""),
                    test_type=tc_data.get("test_type", "functional"),
                    priority=tc_data.get("priority", "medium"),
                    steps=tc_data.get("steps", []),
                    expected_result=tc_data.get("expected_result", ""),
                    preconditions=tc_data.get("preconditions", []),
                    test_data=tc_data.get("test_data", {}),
                    automation_potential=tc_data.get("automation_potential", "medium"),
                    estimated_duration=tc_data.get("estimated_duration", "5-10 minutes")
                )
                test_cases.append(test_case)
        
        # Calcular tiempo de procesamiento
        processing_time = (datetime.utcnow() - start_time).total_seconds()
        
        # Crear respuesta
        response = RequirementsAnalysisResponse(
            requirement_id=request.requirement_id,
            analysis_id=analysis_id,
            status="completed",
            test_cases=test_cases,
            coverage_analysis=analysis_result.get("coverage_analysis", {}),
            confidence_score=analysis_result.get("confidence_score", 0.8),
            processing_time=processing_time,
            created_at=start_time
        )
        
        # Registrar en background task para tracking
        background_tasks.add_task(
            log_requirements_analysis_completion,
            analysis_id,
            request.requirement_id,
            response
        )
        
        logger.info(
            "Requirements analysis completed",
            requirement_id=request.requirement_id,
            analysis_id=analysis_id,
            test_cases_count=len(test_cases),
            processing_time=processing_time
        )
        
        return response
        
    except Exception as e:
        logger.error(
            "Requirements analysis failed",
            requirement_id=request.requirement_id,
            analysis_id=analysis_id,
            error=str(e)
        )
        raise HTTPException(
            status_code=500,
            detail=f"Error analyzing requirements: {str(e)}"
        )

@app.post("/analyze-jira-workitem", 
          response_model=JiraWorkItemResponse,
          summary="Analizar work item de Jira y generar casos de prueba",
          description="Obtiene un work item de Jira y genera casos de prueba estructurados usando IA",
          tags=["Integraci칩n Jira"],
          responses={
              200: {
                  "description": "An치lisis de work item completado exitosamente",
                  "model": JiraWorkItemResponse
              },
              400: {
                  "description": "Datos de entrada inv치lidos o work item no encontrado",
                  "content": {
                      "application/json": {
                          "example": {"detail": "Work item no encontrado en Jira"}
                      }
                  }
              },
              500: {
                  "description": "Error interno del servidor",
                  "content": {
                      "application/json": {
                          "example": {"detail": "Error interno del servidor"}
                      }
                  }
              }
          })
async def analyze_jira_workitem(
    request: JiraWorkItemRequest,
    background_tasks: BackgroundTasks
):
    """
    ## Analizar Work Item de Jira y Generar Casos de Prueba
    
    Obtiene un work item espec칤fico de Jira y genera casos de prueba estructurados basados en su contenido.
    
    ### Proceso:
    1. **Obtenci칩n de Jira**: Se recupera el work item desde Jira API
    2. **Extracci칩n de Datos**: Se extrae informaci칩n relevante (summary, description, acceptance criteria)
    3. **An치lisis IA**: Se procesa con Google Gemini usando prompts especializados
    4. **Generaci칩n de Casos**: Se crean casos de prueba estructurados
    5. **An치lisis de Cobertura**: Se eval칰a la cobertura de pruebas generada
    
    ### Datos Obtenidos de Jira:
    - **Summary**: T칤tulo del work item
    - **Description**: Descripci칩n detallada
    - **Acceptance Criteria**: Criterios de aceptaci칩n (si est치n disponibles)
    - **Issue Type**: Tipo de issue (Story, Task, Bug, etc.)
    - **Priority**: Prioridad del work item
    - **Status**: Estado actual
    
    ### Tipos de Pruebas Soportados:
    - **Functional**: Pruebas funcionales b치sicas
    - **Integration**: Pruebas de integraci칩n
    - **UI**: Pruebas de interfaz de usuario
    - **API**: Pruebas de API
    - **Security**: Pruebas de seguridad
    - **Performance**: Pruebas de rendimiento
    
    ### Respuesta:
    - **jira_data**: Datos completos obtenidos de Jira
    - **test_cases**: Lista de casos de prueba generados
    - **coverage_analysis**: An치lisis de cobertura por tipo
    - **confidence_score**: Puntuaci칩n de confianza (0-1)
    - **processing_time**: Tiempo de procesamiento en segundos
    """
    start_time = datetime.utcnow()
    analysis_id = f"jira_analysis_{request.work_item_id.replace('-', '')}_{int(start_time.timestamp())}"
    
    try:
        logger.info(
            "Starting Jira work item analysis",
            work_item_id=request.work_item_id,
            project_key=request.project_key,
            analysis_id=analysis_id
        )
        
        # Obtener datos del work item desde Jira
        jira_data = await tracker_client.get_work_item_details(
            work_item_id=request.work_item_id,
            project_key=request.project_key
        )
        
        if not jira_data:
            raise HTTPException(
                status_code=404,
                detail=f"Work item {request.work_item_id} not found in project {request.project_key}"
            )
        
        # Construir contenido para an치lisis
        requirement_content = f"""
        T칈TULO: {jira_data.get('summary', '')}
        
        DESCRIPCI칍N:
        {jira_data.get('description', '')}
        
        TIPO DE ISSUE: {jira_data.get('issue_type', '')}
        PRIORIDAD: {jira_data.get('priority', '')}
        ESTADO: {jira_data.get('status', '')}
        """
        
        # Agregar criterios de aceptaci칩n si est치n disponibles
        if request.include_acceptance_criteria and jira_data.get('acceptance_criteria'):
            requirement_content += f"""
            
            CRITERIOS DE ACEPTACI칍N:
            {jira_data.get('acceptance_criteria', '')}
            """
        
        # Sanitizar contenido sensible
        sanitized_content = sanitizer.sanitize(requirement_content)
        
        # Generar prompt para an치lisis de work item
        prompt = prompt_templates.get_jira_workitem_analysis_prompt(
            work_item_data=jira_data,
            requirement_content=sanitized_content,
            project_key=request.project_key,
            test_types=request.test_types,
            coverage_level=request.coverage_level
        )
        
        # Ejecutar an치lisis con LLM
        analysis_result = await llm_wrapper.analyze_jira_workitem(
            prompt=prompt,
            work_item_id=request.work_item_id,
            analysis_id=analysis_id
        )
        
        # Procesar casos de prueba generados
        test_cases = []
        if analysis_result.get("test_cases"):
            for tc_data in analysis_result["test_cases"]:
                test_case = TestCase(
                    test_case_id=tc_data.get("test_case_id", f"TC-{request.project_key}-001"),
                    title=tc_data.get("title", ""),
                    description=tc_data.get("description", ""),
                    test_type=tc_data.get("test_type", "functional"),
                    priority=tc_data.get("priority", "medium"),
                    steps=tc_data.get("steps", []),
                    expected_result=tc_data.get("expected_result", ""),
                    preconditions=tc_data.get("preconditions", []),
                    test_data=tc_data.get("test_data", {}),
                    automation_potential=tc_data.get("automation_potential", "medium"),
                    estimated_duration=tc_data.get("estimated_duration", "5-10 minutes")
                )
                test_cases.append(test_case)
        
        # Calcular tiempo de procesamiento
        processing_time = (datetime.utcnow() - start_time).total_seconds()
        
        # Crear respuesta
        response = JiraWorkItemResponse(
            work_item_id=request.work_item_id,
            jira_data=jira_data,
            analysis_id=analysis_id,
            status="completed",
            test_cases=test_cases,
            coverage_analysis=analysis_result.get("coverage_analysis", {}),
            confidence_score=analysis_result.get("confidence_score", 0.8),
            processing_time=processing_time,
            created_at=start_time
        )
        
        # Registrar en background task para tracking
        background_tasks.add_task(
            log_jira_workitem_analysis_completion,
            analysis_id,
            request.work_item_id,
            response
        )
        
        logger.info(
            "Jira work item analysis completed",
            work_item_id=request.work_item_id,
            analysis_id=analysis_id,
            test_cases_count=len(test_cases),
            processing_time=processing_time
        )
        
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(
            "Jira work item analysis failed",
            work_item_id=request.work_item_id,
            analysis_id=analysis_id,
            error=str(e)
        )
        raise HTTPException(
            status_code=500,
            detail=f"Error analyzing Jira work item: {str(e)}"
        )

@app.get("/analysis/{analysis_id}")
async def get_analysis_result(analysis_id: str):
    """
    Obtener resultado de an치lisis por ID
    """
    try:
        # Aqu칤 implementar칤as la l칩gica para recuperar el an치lisis
        # Por ahora retornamos un placeholder
        return {
            "analysis_id": analysis_id,
            "status": "completed",
            "message": "Analysis result retrieval not implemented yet"
        }
    except Exception as e:
        logger.error("Failed to get analysis result", analysis_id=analysis_id, error=str(e))
        raise HTTPException(status_code=404, detail="Analysis not found")

async def log_analysis_completion(
    analysis_id: str,
    test_case_id: str,
    response: TestCaseAnalysisResponse
):
    """Background task para registrar la finalizaci칩n del an치lisis"""
    try:
        # Aqu칤 podr칤as implementar l칩gica adicional como:
        # - Guardar en base de datos
        # - Enviar notificaciones
        # - Actualizar m칠tricas
        logger.info(
            "Analysis completion logged",
            analysis_id=analysis_id,
            test_case_id=test_case_id
        )
    except Exception as e:
        logger.error(
            "Failed to log analysis completion",
            analysis_id=analysis_id,
            error=str(e)
        )

async def log_requirements_analysis_completion(
    analysis_id: str,
    requirement_id: str,
    response: RequirementsAnalysisResponse
):
    """Background task para registrar la finalizaci칩n del an치lisis de requerimientos"""
    try:
        # Aqu칤 podr칤as implementar l칩gica adicional como:
        # - Guardar en base de datos
        # - Enviar notificaciones
        # - Actualizar m칠tricas
        logger.info(
            "Requirements analysis completion logged",
            analysis_id=analysis_id,
            requirement_id=requirement_id,
            test_cases_count=len(response.test_cases)
        )
    except Exception as e:
        logger.error(
            "Failed to log requirements analysis completion",
            analysis_id=analysis_id,
            error=str(e)
        )

async def log_jira_workitem_analysis_completion(
    analysis_id: str,
    work_item_id: str,
    response: JiraWorkItemResponse
):
    """Background task para registrar la finalizaci칩n del an치lisis de work item de Jira"""
    try:
        # Aqu칤 podr칤as implementar l칩gica adicional como:
        # - Guardar en base de datos
        # - Enviar notificaciones
        # - Actualizar m칠tricas
        # - Crear casos de prueba en Jira
        logger.info(
            "Jira work item analysis completion logged",
            analysis_id=analysis_id,
            work_item_id=work_item_id,
            test_cases_count=len(response.test_cases)
        )
    except Exception as e:
        logger.error(
            "Failed to log Jira work item analysis completion",
            analysis_id=analysis_id,
            error=str(e)
        )

if __name__ == "__main__":
    import uvicorn
    
    port = int(os.getenv("PORT", 8000))
    log_level = os.getenv("LOG_LEVEL", "info").lower()
    
    logger.info("Starting Microservicio de An치lisis QA", port=port, log_level=log_level)
    
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=port,
        reload=True,
        log_level=log_level
    )
