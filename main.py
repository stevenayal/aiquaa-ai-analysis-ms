"""
Microservicio de An√°lisis QA con Langfuse
FastAPI Service para an√°lisis automatizado de casos de prueba
"""

import os
import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime
import structlog
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field
from dotenv import load_dotenv

# Importar m√≥dulos locales
from tracker_client import TrackerClient
from llm_wrapper import LLMWrapper
from prompt_templates import PromptTemplates
from sanitizer import PIISanitizer

# Cargar variables de entorno
load_dotenv()

# Configurar logging estructurado
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()

# Inicializar FastAPI
app = FastAPI(
    title="Microservicio de An√°lisis QA",
    description="""
    ## API de An√°lisis Automatizado de Casos de Prueba con T√©cnicas ISTQB
    
    Esta API proporciona an√°lisis inteligente de casos de prueba utilizando IA generativa y t√©cnicas de dise√±o ISTQB Foundation Level.
    
    ### Caracter√≠sticas:
    - ü§ñ An√°lisis automatizado con Google Gemini
    - üìä Observabilidad completa con Langfuse
    - üîó Integraci√≥n con Jira
    - üìù Sugerencias de mejora estructuradas
    - üöÄ Procesamiento en lote
    - üéØ **NUEVO**: Generaci√≥n de casos con t√©cnicas ISTQB avanzadas
    - üî¨ **NUEVO**: Aplicaci√≥n de 9 t√©cnicas de dise√±o de pruebas
    - üìã **NUEVO**: Formato estructurado con CSV, fichas y artefactos t√©cnicos
    
    ### T√©cnicas ISTQB Soportadas:
    - **Equivalencia**: Partici√≥n de clases de equivalencia
    - **Valores L√≠mite**: An√°lisis de valores l√≠mite
    - **Tabla de Decisi√≥n**: Matrices de condiciones y acciones
    - **Transici√≥n de Estados**: Estados y transiciones del sistema
    - **√Årbol de Clasificaci√≥n**: Clases y restricciones
    - **Pairwise**: Combinaciones m√≠nimas de pares
    - **Casos de Uso**: Flujos principales y alternos
    - **Error Guessing**: Hip√≥tesis de fallos
    - **Checklist**: Verificaci√≥n gen√©rica de calidad
    
    ### Autenticaci√≥n:
    No se requiere autenticaci√≥n para las pruebas locales.
    
    ### Uso:
    1. **An√°lisis b√°sico**: Env√≠a un caso de prueba al endpoint `/analyze`
    2. **Generaci√≥n ISTQB**: Usa `/generate-istqb-tests` para casos avanzados
    3. **An√°lisis de requerimientos**: Usa `/analyze-requirements` para generar casos
    4. **Integraci√≥n Jira**: Usa `/analyze-jira-workitem` para work items
    5. **Monitoreo**: Verifica el estado con `/health`
    """,
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_url="/openapi.json",
    contact={
        "name": "Equipo de QA",
        "email": "qa-team@company.com",
    },
    license_info={
        "name": "MIT",
    },
    servers=[
        {
            "url": "http://localhost:8000",
            "description": "Servidor de desarrollo local"
        }
    ]
)

# Inicializar componentes
tracker_client = TrackerClient()
llm_wrapper = LLMWrapper()
prompt_templates = PromptTemplates()
sanitizer = PIISanitizer()

# Modelos Pydantic
class TestCaseAnalysisRequest(BaseModel):
    """Solicitud de an√°lisis de caso de prueba"""
    test_case_id: str = Field(
        ..., 
        description="ID √∫nico del caso de prueba",
        example="TC-001",
        min_length=1,
        max_length=50
    )
    test_case_content: str = Field(
        ..., 
        description="Descripci√≥n detallada del caso de prueba a analizar",
        example="Verificar que el usuario pueda iniciar sesi√≥n con credenciales v√°lidas",
        min_length=10,
        max_length=5000
    )
    project_key: str = Field(
        ..., 
        description="Clave del proyecto en Jira",
        example="TEST",
        min_length=1,
        max_length=20
    )
    priority: Optional[str] = Field(
        "Medium", 
        description="Prioridad del caso de prueba",
        example="High",
        pattern="^(Low|Medium|High|Critical)$"
    )
    labels: Optional[List[str]] = Field(
        default_factory=list, 
        description="Etiquetas para categorizar el caso de prueba",
        example=["login", "authentication", "smoke-test"]
    )
    
    class Config:
        schema_extra = {
            "example": {
                "test_case_id": "TC-001",
                "test_case_content": "Verificar que el usuario pueda iniciar sesi√≥n con credenciales v√°lidas. Pasos: 1) Abrir la p√°gina de login, 2) Ingresar usuario v√°lido, 3) Ingresar contrase√±a v√°lida, 4) Hacer clic en 'Iniciar Sesi√≥n'. Resultado esperado: Usuario logueado exitosamente y redirigido al dashboard.",
                "project_key": "TEST",
                "priority": "High",
                "labels": ["login", "authentication", "smoke-test"]
            }
        }

class Suggestion(BaseModel):
    """Sugerencia de mejora para un caso de prueba"""
    type: str = Field(..., description="Tipo de sugerencia", example="clarity")
    title: str = Field(..., description="T√≠tulo de la sugerencia", example="Definir datos de prueba espec√≠ficos")
    description: str = Field(..., description="Descripci√≥n detallada", example="El caso de prueba debe incluir datos espec√≠ficos de usuario y contrase√±a")
    priority: str = Field(..., description="Prioridad de la sugerencia", example="high")
    category: str = Field(..., description="Categor√≠a de la mejora", example="improvement")

class TestCaseAnalysisResponse(BaseModel):
    """Respuesta del an√°lisis de caso de prueba"""
    test_case_id: str = Field(..., description="ID del caso de prueba analizado", example="TC-001")
    analysis_id: str = Field(..., description="ID √∫nico del an√°lisis", example="analysis_TC001_1760825804")
    status: str = Field(..., description="Estado del an√°lisis", example="completed")
    suggestions: List[Suggestion] = Field(..., description="Lista de sugerencias de mejora")
    confidence_score: float = Field(..., description="Puntuaci√≥n de confianza del an√°lisis (0-1)", example=0.85)
    processing_time: float = Field(..., description="Tiempo de procesamiento en segundos", example=8.81)
    created_at: datetime = Field(..., description="Timestamp de creaci√≥n del an√°lisis")
    
    class Config:
        schema_extra = {
            "example": {
                "test_case_id": "TC-001",
                "analysis_id": "analysis_TC001_1760825804",
                "status": "completed",
                "suggestions": [
                    {
                        "type": "clarity",
                        "title": "Definir datos de prueba espec√≠ficos",
                        "description": "El caso de prueba debe incluir datos espec√≠ficos de usuario y contrase√±a",
                        "priority": "high",
                        "category": "improvement"
                    }
                ],
                "confidence_score": 0.85,
                "processing_time": 8.81,
                "created_at": "2025-10-18T19:16:44.520862"
            }
        }

class RequirementsAnalysisRequest(BaseModel):
    """Solicitud de an√°lisis de requerimientos para generar casos de prueba"""
    requirement_id: str = Field(
        ..., 
        description="ID √∫nico del requerimiento",
        example="REQ-001",
        min_length=1,
        max_length=50
    )
    requirement_content: str = Field(
        ..., 
        description="Descripci√≥n detallada del requerimiento a analizar",
        example="El sistema debe permitir a los usuarios autenticarse usando email y contrase√±a",
        min_length=10,
        max_length=10000
    )
    project_key: str = Field(
        ..., 
        description="Clave del proyecto en Jira",
        example="AUTH",
        min_length=1,
        max_length=20
    )
    priority: Optional[str] = Field(
        "Medium", 
        description="Prioridad del requerimiento",
        example="High",
        pattern="^(Low|Medium|High|Critical)$"
    )
    test_types: Optional[List[str]] = Field(
        default_factory=lambda: ["functional", "integration"],
        description="Tipos de pruebas a generar",
        example=["functional", "integration", "ui", "api", "security"]
    )
    coverage_level: Optional[str] = Field(
        "medium",
        description="Nivel de cobertura de pruebas",
        example="high",
        pattern="^(low|medium|high|comprehensive)$"
    )
    
    class Config:
        json_schema_extra = {
            "example": {
                "requirement_id": "REQ-001",
                "requirement_content": "El sistema debe permitir a los usuarios autenticarse usando email y contrase√±a. El sistema debe validar las credenciales contra la base de datos y permitir el acceso solo a usuarios activos. En caso de credenciales incorrectas, debe mostrar un mensaje de error apropiado.",
                "project_key": "AUTH",
                "priority": "High",
                "test_types": ["functional", "integration", "security"],
                "coverage_level": "high"
            }
        }

class TestCase(BaseModel):
    """Caso de prueba generado"""
    test_case_id: str = Field(..., description="ID del caso de prueba", example="TC-AUTH-001")
    title: str = Field(..., description="T√≠tulo del caso de prueba", example="Verificar login con credenciales v√°lidas")
    description: str = Field(..., description="Descripci√≥n detallada del caso de prueba")
    test_type: str = Field(..., description="Tipo de prueba", example="functional")
    priority: str = Field(..., description="Prioridad del caso de prueba", example="high")
    steps: List[str] = Field(..., description="Pasos detallados del caso de prueba")
    expected_result: str = Field(..., description="Resultado esperado")
    preconditions: List[str] = Field(default_factory=list, description="Precondiciones necesarias")
    test_data: Optional[Dict[str, Any]] = Field(default_factory=dict, description="Datos de prueba espec√≠ficos")
    automation_potential: str = Field(..., description="Potencial de automatizaci√≥n", example="high")
    estimated_duration: str = Field(..., description="Duraci√≥n estimada", example="5-10 minutes")

class RequirementsAnalysisResponse(BaseModel):
    """Respuesta del an√°lisis de requerimientos"""
    requirement_id: str = Field(..., description="ID del requerimiento analizado", example="REQ-001")
    analysis_id: str = Field(..., description="ID √∫nico del an√°lisis", example="req_analysis_REQ001_1760825804")
    status: str = Field(..., description="Estado del an√°lisis", example="completed")
    test_cases: List[TestCase] = Field(..., description="Lista de casos de prueba generados")
    coverage_analysis: Dict[str, Any] = Field(..., description="An√°lisis de cobertura de pruebas")
    confidence_score: float = Field(..., description="Puntuaci√≥n de confianza del an√°lisis (0-1)", example=0.85)
    processing_time: float = Field(..., description="Tiempo de procesamiento en segundos", example=12.5)
    created_at: datetime = Field(..., description="Timestamp de creaci√≥n del an√°lisis")
    
    class Config:
        json_schema_extra = {
            "example": {
                "requirement_id": "REQ-001",
                "analysis_id": "req_analysis_REQ001_1760825804",
                "status": "completed",
                "test_cases": [
                    {
                        "test_case_id": "TC-AUTH-001",
                        "title": "Verificar login con credenciales v√°lidas",
                        "description": "Caso de prueba para verificar que un usuario puede autenticarse exitosamente",
                        "test_type": "functional",
                        "priority": "high",
                        "steps": [
                            "Navegar a la p√°gina de login",
                            "Ingresar email v√°lido",
                            "Ingresar contrase√±a v√°lida",
                            "Hacer clic en 'Iniciar Sesi√≥n'"
                        ],
                        "expected_result": "Usuario autenticado exitosamente y redirigido al dashboard",
                        "preconditions": ["Usuario existe en la base de datos", "Usuario est√° activo"],
                        "test_data": {"email": "test@example.com", "password": "Test123!"},
                        "automation_potential": "high",
                        "estimated_duration": "5-10 minutes"
                    }
                ],
                "coverage_analysis": {
                    "functional_coverage": "90%",
                    "edge_case_coverage": "75%",
                    "integration_coverage": "80%"
                },
                "confidence_score": 0.85,
                "processing_time": 12.5,
                "created_at": "2025-10-18T19:16:44.520862"
            }
        }

class JiraWorkItemRequest(BaseModel):
    """Solicitud de an√°lisis de work item de Jira"""
    work_item_id: str = Field(
        ..., 
        description="ID del work item en Jira (ej: PROJ-123)",
        example="AUTH-123",
        min_length=1,
        max_length=50
    )
    project_key: str = Field(
        ..., 
        description="Clave del proyecto en Jira",
        example="AUTH",
        min_length=1,
        max_length=20
    )
    test_types: Optional[List[str]] = Field(
        default_factory=lambda: ["functional", "integration"],
        description="Tipos de pruebas a generar",
        example=["functional", "integration", "ui", "api", "security"]
    )
    coverage_level: Optional[str] = Field(
        "medium",
        description="Nivel de cobertura de pruebas",
        example="high",
        pattern="^(low|medium|high|comprehensive)$"
    )
    include_acceptance_criteria: Optional[bool] = Field(
        True,
        description="Incluir criterios de aceptaci√≥n en el an√°lisis",
        example=True
    )
    
    class Config:
        json_schema_extra = {
            "example": {
                "work_item_id": "AUTH-123",
                "project_key": "AUTH",
                "test_types": ["functional", "integration", "security"],
                "coverage_level": "high",
                "include_acceptance_criteria": True
            }
        }

class JiraWorkItemResponse(BaseModel):
    """Respuesta del an√°lisis de work item de Jira"""
    work_item_id: str = Field(..., description="ID del work item analizado", example="AUTH-123")
    jira_data: Dict[str, Any] = Field(..., description="Datos obtenidos de Jira")
    analysis_id: str = Field(..., description="ID √∫nico del an√°lisis", example="jira_analysis_AUTH123_1760825804")
    status: str = Field(..., description="Estado del an√°lisis", example="completed")
    test_cases: List[TestCase] = Field(..., description="Lista de casos de prueba generados")
    coverage_analysis: Dict[str, Any] = Field(..., description="An√°lisis de cobertura de pruebas")
    confidence_score: float = Field(..., description="Puntuaci√≥n de confianza del an√°lisis (0-1)", example=0.85)
    processing_time: float = Field(..., description="Tiempo de procesamiento en segundos", example=15.5)
    created_at: datetime = Field(..., description="Timestamp de creaci√≥n del an√°lisis")
    
    class Config:
        json_schema_extra = {
            "example": {
                "work_item_id": "AUTH-123",
                "jira_data": {
                    "summary": "Implementar autenticaci√≥n de usuarios",
                    "description": "El sistema debe permitir a los usuarios autenticarse...",
                    "issue_type": "Story",
                    "priority": "High",
                    "status": "In Progress"
                },
                "analysis_id": "jira_analysis_AUTH123_1760825804",
                "status": "completed",
                "test_cases": [
                    {
                        "test_case_id": "TC-AUTH-001",
                        "title": "Verificar login con credenciales v√°lidas",
                        "description": "Caso de prueba para verificar autenticaci√≥n exitosa",
                        "test_type": "functional",
                        "priority": "high",
                        "steps": ["Navegar a login", "Ingresar credenciales", "Hacer clic en login"],
                        "expected_result": "Usuario autenticado exitosamente",
                        "preconditions": ["Usuario existe en BD"],
                        "test_data": {"email": "test@example.com", "password": "Test123!"},
                        "automation_potential": "high",
                        "estimated_duration": "5-10 minutes"
                    }
                ],
                "coverage_analysis": {
                    "functional_coverage": "90%",
                    "edge_case_coverage": "75%",
                    "integration_coverage": "80%"
                },
                "confidence_score": 0.85,
                "processing_time": 15.5,
                "created_at": "2025-10-18T19:16:44.520862"
            }
        }

class ISTQBTestGenerationRequest(BaseModel):
    """Solicitud de generaci√≥n de casos de prueba con t√©cnicas ISTQB"""
    programa: str = Field(
        ..., 
        description="Nombre del sistema/programa",
        example="SISTEMA_AUTH",
        min_length=1,
        max_length=50
    )
    dominio: str = Field(
        ..., 
        description="Breve descripci√≥n del requerimiento",
        example="Autenticaci√≥n de usuarios con validaci√≥n de credenciales",
        min_length=10,
        max_length=200
    )
    modulos: List[str] = Field(
        ..., 
        description="Lista de m√≥dulos del sistema",
        example=["AUTORIZACION", "VALIDACION", "AUDITORIA"],
        min_items=1,
        max_items=10
    )
    factores: Dict[str, List[str]] = Field(
        ..., 
        description="Factores de prueba con sus valores posibles",
        example={
            "TIPO_USUARIO": ["ADMIN", "USER", "GUEST"],
            "ESTADO_CREDENCIAL": ["VALIDA", "INVALIDA", "EXPIRADA"],
            "INTENTOS": ["OK", "ERROR_TIPO_1", "TIMEOUT"]
        }
    )
    limites: Dict[str, Any] = Field(
        ..., 
        description="L√≠mites del sistema",
        example={
            "CAMPO_USUARIO_len": {"min": 1, "max": 64},
            "REINTENTOS": 3,
            "TIMEOUT_MS": 5000
        }
    )
    reglas: List[str] = Field(
        ..., 
        description="Reglas de negocio",
        example=[
            "R1: si TIPO_USUARIO=ADMIN y ESTADO_CREDENCIAL=VALIDA -> ACCESO_TOTAL",
            "R2: si INTENTOS=TIMEOUT -> reintentar 1 vez y marcar pendiente",
            "R3: si REINTENTOS supera l√≠mite -> bloquear y auditar"
        ],
        min_items=1
    )
    tecnicas: Dict[str, bool] = Field(
        ..., 
        description="T√©cnicas ISTQB a aplicar",
        example={
            "equivalencia": True,
            "valores_limite": True,
            "tabla_decision": True,
            "transicion_estados": True,
            "arbol_clasificacion": True,
            "pairwise": True,
            "casos_uso": True,
            "error_guessing": True,
            "checklist": True
        }
    )
    priorizacion: Optional[str] = Field(
        "Riesgo", 
        description="Criterio de priorizaci√≥n",
        example="Riesgo",
        pattern="^(Riesgo|Impacto|Uso)$"
    )
    cantidad_max: Optional[int] = Field(
        150, 
        description="Cantidad m√°xima de casos de prueba a generar",
        example=150,
        ge=10,
        le=500
    )
    salida_plan_ejecucion: Optional[Dict[str, Any]] = Field(
        default_factory=lambda: {"incluir": True, "formato": "cursor_playwright_mcp"},
        description="Configuraci√≥n del plan de ejecuci√≥n"
    )
    
    class Config:
        json_schema_extra = {
            "example": {
                "programa": "SISTEMA_AUTH",
                "dominio": "Autenticaci√≥n de usuarios con validaci√≥n de credenciales",
                "modulos": ["AUTORIZACION", "VALIDACION", "AUDITORIA"],
                "factores": {
                    "TIPO_USUARIO": ["ADMIN", "USER", "GUEST"],
                    "ESTADO_CREDENCIAL": ["VALIDA", "INVALIDA", "EXPIRADA"],
                    "INTENTOS": ["OK", "ERROR_TIPO_1", "TIMEOUT"]
                },
                "limites": {
                    "CAMPO_USUARIO_len": {"min": 1, "max": 64},
                    "REINTENTOS": 3,
                    "TIMEOUT_MS": 5000
                },
                "reglas": [
                    "R1: si TIPO_USUARIO=ADMIN y ESTADO_CREDENCIAL=VALIDA -> ACCESO_TOTAL",
                    "R2: si INTENTOS=TIMEOUT -> reintentar 1 vez y marcar pendiente",
                    "R3: si REINTENTOS supera l√≠mite -> bloquear y auditar"
                ],
                "tecnicas": {
                    "equivalencia": True,
                    "valores_limite": True,
                    "tabla_decision": True,
                    "transicion_estados": True,
                    "arbol_clasificacion": True,
                    "pairwise": True,
                    "casos_uso": True,
                    "error_guessing": True,
                    "checklist": True
                },
                "priorizacion": "Riesgo",
                "cantidad_max": 150,
                "salida_plan_ejecucion": {
                    "incluir": True,
                    "formato": "cursor_playwright_mcp"
                }
            }
        }

class ISTQBTestGenerationResponse(BaseModel):
    """Respuesta de la generaci√≥n de casos de prueba ISTQB"""
    programa: str = Field(..., description="Nombre del programa", example="SISTEMA_AUTH")
    generation_id: str = Field(..., description="ID √∫nico de la generaci√≥n", example="istqb_SISTEMA_AUTH_1760825804")
    status: str = Field(..., description="Estado de la generaci√≥n", example="completed")
    csv_cases: List[str] = Field(..., description="Lista de casos de prueba en formato CSV")
    fichas: List[str] = Field(..., description="Fichas detalladas de casos de prueba")
    artefactos_tecnicos: Dict[str, Any] = Field(..., description="Artefactos t√©cnicos generados")
    plan_ejecucion: Dict[str, Any] = Field(..., description="Plan de ejecuci√≥n (si aplica)")
    confidence_score: float = Field(..., description="Puntuaci√≥n de confianza (0-1)", example=0.85)
    processing_time: float = Field(..., description="Tiempo de procesamiento en segundos", example=25.3)
    created_at: datetime = Field(..., description="Timestamp de creaci√≥n")
    
    class Config:
        json_schema_extra = {
            "example": {
                "programa": "SISTEMA_AUTH",
                "generation_id": "istqb_SISTEMA_AUTH_1760825804",
                "status": "completed",
                "csv_cases": [
                    "CP - 001 - SISTEMA_AUTH - AUTORIZACION - TIPO_USUARIO_ADMIN - AUTORIZA Y REGISTRA OPERACION",
                    "CP - 002 - SISTEMA_AUTH - VALIDACION - ESTADO_CREDENCIAL_VALIDA - VALIDA Y PERMITE ACCESO"
                ],
                "fichas": [
                    "1 - CP - 001 - SISTEMA_AUTH - AUTORIZACION - TIPO_USUARIO_ADMIN - AUTORIZA Y REGISTRA OPERACION\n2- Precondicion: Usuario activo; datos completos; firma v√°lida\n3- Resultado Esperado: Operaci√≥n autorizada; ID transacci√≥n generado; registro persistido y auditado"
                ],
                "artefactos_tecnicos": {
                    "equivalencias": "Particiones v√°lidas/inv√°lidas por cada factor",
                    "valores_limite": "Casos min-1,min,min+1,max-1,max,max+1 para l√≠mites",
                    "tabla_decision": "Matriz Condiciones‚ÜíAcciones"
                },
                "plan_ejecucion": {
                    "formato": "cursor_playwright_mcp",
                    "casos": []
                },
                "confidence_score": 0.85,
                "processing_time": 25.3,
                "created_at": "2025-10-18T19:16:44.520862"
            }
        }

class HealthResponse(BaseModel):
    """Respuesta de salud del servicio"""
    status: str
    timestamp: datetime
    version: str
    components: Dict[str, str]

@app.get("/", 
         response_model=Dict[str, str],
         summary="Informaci√≥n del servicio",
         description="Endpoint ra√≠z que proporciona informaci√≥n b√°sica sobre el microservicio de an√°lisis QA",
         tags=["Informaci√≥n"])
async def root():
    """
    ## Informaci√≥n del Servicio
    
    Retorna informaci√≥n b√°sica sobre el microservicio de an√°lisis QA.
    
    ### Respuesta:
    - **message**: Descripci√≥n del servicio
    - **version**: Versi√≥n actual de la API
    - **docs**: URL de la documentaci√≥n Swagger
    """
    return {
        "message": "Microservicio de An√°lisis QA",
        "version": "1.0.0",
        "docs": "/docs"
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Verificaci√≥n de salud del servicio"""
    components = {}
    
    # Verificar Langfuse
    try:
        await llm_wrapper.health_check()
        components["langfuse"] = "healthy"
    except Exception as e:
        logger.error("Langfuse health check failed", error=str(e))
        components["langfuse"] = "unhealthy"
    
    # Verificar Jira
    try:
        await tracker_client.health_check()
        components["jira"] = "healthy"
    except Exception as e:
        logger.error("Jira health check failed", error=str(e))
        components["jira"] = "unhealthy"
    
    # Verificar LLM
    try:
        await llm_wrapper.test_connection()
        components["llm"] = "healthy"
    except Exception as e:
        logger.error("LLM health check failed", error=str(e))
        components["llm"] = "unhealthy"
    
    overall_status = "healthy" if all(status == "healthy" for status in components.values()) else "degraded"
    
    return HealthResponse(
        status=overall_status,
        timestamp=datetime.utcnow(),
        version="1.0.0",
        components=components
    )

@app.post("/analyze", 
          response_model=TestCaseAnalysisResponse,
          summary="Analizar caso de prueba",
          description="Analiza un caso de prueba individual y genera sugerencias de mejora usando IA",
          tags=["An√°lisis"],
          responses={
              200: {
                  "description": "An√°lisis completado exitosamente",
                  "model": TestCaseAnalysisResponse
              },
              400: {
                  "description": "Datos de entrada inv√°lidos",
                  "content": {
                      "application/json": {
                          "example": {"detail": "Datos de entrada inv√°lidos"}
                      }
                  }
              },
              500: {
                  "description": "Error interno del servidor",
                  "content": {
                      "application/json": {
                          "example": {"detail": "Error interno del servidor"}
                      }
                  }
              }
          })
async def analyze_test_case(
    request: TestCaseAnalysisRequest,
    background_tasks: BackgroundTasks
):
    """
    ## Analizar Caso de Prueba
    
    Analiza un caso de prueba individual utilizando IA generativa (Google Gemini) y genera sugerencias de mejora estructuradas.
    
    ### Proceso:
    1. **Sanitizaci√≥n**: Se elimina informaci√≥n sensible del contenido
    2. **An√°lisis IA**: Se procesa con Google Gemini usando prompts especializados
    3. **Estructuraci√≥n**: Se organizan las sugerencias en categor√≠as
    4. **Observabilidad**: Se registra en Langfuse para monitoreo
    
    ### Tipos de Sugerencias:
    - **Clarity**: Mejoras en claridad y legibilidad
    - **Coverage**: Sugerencias para mejorar cobertura de pruebas
    - **Automation**: Optimizaciones para automatizaci√≥n
    - **Best Practice**: Mejores pr√°cticas de testing
    
    ### Respuesta:
    - **suggestions**: Lista de sugerencias categorizadas
    - **confidence_score**: Puntuaci√≥n de confianza (0-1)
    - **processing_time**: Tiempo de procesamiento en segundos
    """
    start_time = datetime.utcnow()
    analysis_id = f"analysis_{request.test_case_id}_{int(start_time.timestamp())}"
    
    try:
        logger.info(
            "Starting test case analysis",
            test_case_id=request.test_case_id,
            analysis_id=analysis_id,
            project_key=request.project_key
        )
        
        # Sanitizar contenido sensible
        sanitized_content = sanitizer.sanitize(request.test_case_content)
        
        # Obtener prompt para an√°lisis
        prompt = prompt_templates.get_analysis_prompt(
            test_case_content=sanitized_content,
            project_key=request.project_key,
            priority=request.priority,
            labels=request.labels
        )
        
        # Ejecutar an√°lisis con LLM
        analysis_result = await llm_wrapper.analyze_test_case(
            prompt=prompt,
            test_case_id=request.test_case_id,
            analysis_id=analysis_id
        )
        
        # Procesar sugerencias
        suggestions = []
        if analysis_result.get("suggestions"):
            for suggestion in analysis_result["suggestions"]:
                suggestions.append({
                    "type": suggestion.get("type", "general"),
                    "title": suggestion.get("title", ""),
                    "description": suggestion.get("description", ""),
                    "priority": suggestion.get("priority", "medium"),
                    "category": suggestion.get("category", "improvement")
                })
        
        # Calcular tiempo de procesamiento
        processing_time = (datetime.utcnow() - start_time).total_seconds()
        
        # Crear respuesta
        response = TestCaseAnalysisResponse(
            test_case_id=request.test_case_id,
            analysis_id=analysis_id,
            status="completed",
            suggestions=suggestions,
            confidence_score=analysis_result.get("confidence_score", 0.8),
            processing_time=processing_time,
            created_at=start_time
        )
        
        # Registrar en background task para tracking
        background_tasks.add_task(
            log_analysis_completion,
            analysis_id,
            request.test_case_id,
            response
        )
        
        logger.info(
            "Test case analysis completed",
            test_case_id=request.test_case_id,
            analysis_id=analysis_id,
            suggestions_count=len(suggestions),
            processing_time=processing_time
        )
        
        return response
        
    except Exception as e:
        logger.error(
            "Test case analysis failed",
            test_case_id=request.test_case_id,
            analysis_id=analysis_id,
            error=str(e)
        )
        raise HTTPException(
            status_code=500,
            detail=f"Error analyzing test case: {str(e)}"
        )

@app.post("/batch-analyze")
async def batch_analyze_test_cases(
    requests: List[TestCaseAnalysisRequest],
    background_tasks: BackgroundTasks
):
    """
    Analizar m√∫ltiples casos de prueba en lote
    """
    logger.info("Starting batch analysis", count=len(requests))
    
    results = []
    for request in requests:
        try:
            result = await analyze_test_case(request, background_tasks)
            results.append(result)
        except Exception as e:
            logger.error(
                "Failed to analyze test case in batch",
                test_case_id=request.test_case_id,
                error=str(e)
            )
            results.append({
                "test_case_id": request.test_case_id,
                "status": "failed",
                "error": str(e)
            })
    
    return {
        "total_processed": len(requests),
        "successful": len([r for r in results if r.get("status") == "completed"]),
        "failed": len([r for r in results if r.get("status") == "failed"]),
        "results": results
    }

@app.post("/analyze-requirements", 
          response_model=RequirementsAnalysisResponse,
          summary="Analizar requerimientos y generar casos de prueba",
          description="Analiza un requerimiento y genera casos de prueba estructurados usando IA",
          tags=["Requerimientos"],
          responses={
              200: {
                  "description": "An√°lisis de requerimientos completado exitosamente",
                  "model": RequirementsAnalysisResponse
              },
              400: {
                  "description": "Datos de entrada inv√°lidos",
                  "content": {
                      "application/json": {
                          "example": {"detail": "Datos de entrada inv√°lidos"}
                      }
                  }
              },
              500: {
                  "description": "Error interno del servidor",
                  "content": {
                      "application/json": {
                          "example": {"detail": "Error interno del servidor"}
                      }
                  }
              }
          })
async def analyze_requirements(
    request: RequirementsAnalysisRequest,
    background_tasks: BackgroundTasks
):
    """
    ## Analizar Requerimientos y Generar Casos de Prueba
    
    Analiza un requerimiento de software y genera casos de prueba estructurados utilizando IA generativa.
    
    ### Proceso:
    1. **An√°lisis del Requerimiento**: Se procesa el contenido del requerimiento
    2. **Generaci√≥n de Casos**: Se crean casos de prueba para diferentes tipos y niveles
    3. **Estructuraci√≥n**: Se organizan los casos con pasos, datos y resultados esperados
    4. **An√°lisis de Cobertura**: Se eval√∫a la cobertura de pruebas generada
    
    ### Tipos de Pruebas Soportados:
    - **Functional**: Pruebas funcionales b√°sicas
    - **Integration**: Pruebas de integraci√≥n
    - **UI**: Pruebas de interfaz de usuario
    - **API**: Pruebas de API
    - **Security**: Pruebas de seguridad
    - **Performance**: Pruebas de rendimiento
    
    ### Niveles de Cobertura:
    - **Low**: Casos b√°sicos esenciales
    - **Medium**: Casos est√°ndar con casos edge
    - **High**: Cobertura completa con casos complejos
    - **Comprehensive**: Cobertura exhaustiva con todos los escenarios
    
    ### Respuesta:
    - **test_cases**: Lista de casos de prueba generados
    - **coverage_analysis**: An√°lisis de cobertura por tipo
    - **confidence_score**: Puntuaci√≥n de confianza (0-1)
    - **processing_time**: Tiempo de procesamiento en segundos
    """
    start_time = datetime.utcnow()
    analysis_id = f"req_analysis_{request.requirement_id}_{int(start_time.timestamp())}"
    
    try:
        logger.info(
            "Starting requirements analysis",
            requirement_id=request.requirement_id,
            analysis_id=analysis_id,
            project_key=request.project_key
        )
        
        # Sanitizar contenido sensible
        sanitized_content = sanitizer.sanitize(request.requirement_content)
        
        # Generar prompt para an√°lisis de requerimientos
        prompt = prompt_templates.get_requirements_analysis_prompt(
            requirement_content=sanitized_content,
            project_key=request.project_key,
            priority=request.priority,
            test_types=request.test_types,
            coverage_level=request.coverage_level
        )
        
        # Ejecutar an√°lisis con LLM
        analysis_result = await llm_wrapper.analyze_requirements(
            prompt=prompt,
            requirement_id=request.requirement_id,
            analysis_id=analysis_id
        )
        
        # Procesar casos de prueba generados
        test_cases = []
        if analysis_result.get("test_cases"):
            for tc_data in analysis_result["test_cases"]:
                test_case = TestCase(
                    test_case_id=tc_data.get("test_case_id", f"TC-{request.requirement_id}-001"),
                    title=tc_data.get("title", ""),
                    description=tc_data.get("description", ""),
                    test_type=tc_data.get("test_type", "functional"),
                    priority=tc_data.get("priority", "medium"),
                    steps=tc_data.get("steps", []),
                    expected_result=tc_data.get("expected_result", ""),
                    preconditions=tc_data.get("preconditions", []),
                    test_data=tc_data.get("test_data", {}),
                    automation_potential=tc_data.get("automation_potential", "medium"),
                    estimated_duration=tc_data.get("estimated_duration", "5-10 minutes")
                )
                test_cases.append(test_case)
        
        # Calcular tiempo de procesamiento
        processing_time = (datetime.utcnow() - start_time).total_seconds()
        
        # Crear respuesta
        response = RequirementsAnalysisResponse(
            requirement_id=request.requirement_id,
            analysis_id=analysis_id,
            status="completed",
            test_cases=test_cases,
            coverage_analysis=analysis_result.get("coverage_analysis", {}),
            confidence_score=analysis_result.get("confidence_score", 0.8),
            processing_time=processing_time,
            created_at=start_time
        )
        
        # Registrar en background task para tracking
        background_tasks.add_task(
            log_requirements_analysis_completion,
            analysis_id,
            request.requirement_id,
            response
        )
        
        logger.info(
            "Requirements analysis completed",
            requirement_id=request.requirement_id,
            analysis_id=analysis_id,
            test_cases_count=len(test_cases),
            processing_time=processing_time
        )
        
        return response
        
    except Exception as e:
        logger.error(
            "Requirements analysis failed",
            requirement_id=request.requirement_id,
            analysis_id=analysis_id,
            error=str(e)
        )
        raise HTTPException(
            status_code=500,
            detail=f"Error analyzing requirements: {str(e)}"
        )

@app.post("/analyze-jira-workitem", 
          response_model=JiraWorkItemResponse,
          summary="Analizar work item de Jira y generar casos de prueba",
          description="Obtiene un work item de Jira y genera casos de prueba estructurados usando IA",
          tags=["Integraci√≥n Jira"],
          responses={
              200: {
                  "description": "An√°lisis de work item completado exitosamente",
                  "model": JiraWorkItemResponse
              },
              400: {
                  "description": "Datos de entrada inv√°lidos o work item no encontrado",
                  "content": {
                      "application/json": {
                          "example": {"detail": "Work item no encontrado en Jira"}
                      }
                  }
              },
              500: {
                  "description": "Error interno del servidor",
                  "content": {
                      "application/json": {
                          "example": {"detail": "Error interno del servidor"}
                      }
                  }
              }
          })
async def analyze_jira_workitem(
    request: JiraWorkItemRequest,
    background_tasks: BackgroundTasks
):
    """
    ## Analizar Work Item de Jira y Generar Casos de Prueba
    
    Obtiene un work item espec√≠fico de Jira y genera casos de prueba estructurados basados en su contenido.
    
    ### Proceso:
    1. **Obtenci√≥n de Jira**: Se recupera el work item desde Jira API
    2. **Extracci√≥n de Datos**: Se extrae informaci√≥n relevante (summary, description, acceptance criteria)
    3. **An√°lisis IA**: Se procesa con Google Gemini usando prompts especializados
    4. **Generaci√≥n de Casos**: Se crean casos de prueba estructurados
    5. **An√°lisis de Cobertura**: Se eval√∫a la cobertura de pruebas generada
    
    ### Datos Obtenidos de Jira:
    - **Summary**: T√≠tulo del work item
    - **Description**: Descripci√≥n detallada
    - **Acceptance Criteria**: Criterios de aceptaci√≥n (si est√°n disponibles)
    - **Issue Type**: Tipo de issue (Story, Task, Bug, etc.)
    - **Priority**: Prioridad del work item
    - **Status**: Estado actual
    
    ### Tipos de Pruebas Soportados:
    - **Functional**: Pruebas funcionales b√°sicas
    - **Integration**: Pruebas de integraci√≥n
    - **UI**: Pruebas de interfaz de usuario
    - **API**: Pruebas de API
    - **Security**: Pruebas de seguridad
    - **Performance**: Pruebas de rendimiento
    
    ### Respuesta:
    - **jira_data**: Datos completos obtenidos de Jira
    - **test_cases**: Lista de casos de prueba generados
    - **coverage_analysis**: An√°lisis de cobertura por tipo
    - **confidence_score**: Puntuaci√≥n de confianza (0-1)
    - **processing_time**: Tiempo de procesamiento en segundos
    """
    start_time = datetime.utcnow()
    analysis_id = f"jira_analysis_{request.work_item_id.replace('-', '')}_{int(start_time.timestamp())}"
    
    try:
        logger.info(
            "Starting Jira work item analysis",
            work_item_id=request.work_item_id,
            project_key=request.project_key,
            analysis_id=analysis_id
        )
        
        # Obtener datos del work item desde Jira
        jira_data = await tracker_client.get_work_item_details(
            work_item_id=request.work_item_id,
            project_key=request.project_key
        )
        
        if not jira_data:
            raise HTTPException(
                status_code=404,
                detail=f"Work item {request.work_item_id} not found in project {request.project_key}"
            )
        
        # Construir contenido para an√°lisis
        requirement_content = f"""
        T√çTULO: {jira_data.get('summary', '')}
        
        DESCRIPCI√ìN:
        {jira_data.get('description', '')}
        
        TIPO DE ISSUE: {jira_data.get('issue_type', '')}
        PRIORIDAD: {jira_data.get('priority', '')}
        ESTADO: {jira_data.get('status', '')}
        """
        
        # Agregar criterios de aceptaci√≥n si est√°n disponibles
        if request.include_acceptance_criteria and jira_data.get('acceptance_criteria'):
            requirement_content += f"""
            
            CRITERIOS DE ACEPTACI√ìN:
            {jira_data.get('acceptance_criteria', '')}
            """
        
        # Sanitizar contenido sensible
        sanitized_content = sanitizer.sanitize(requirement_content)
        
        # Generar prompt para an√°lisis de work item
        prompt = prompt_templates.get_jira_workitem_analysis_prompt(
            work_item_data=jira_data,
            requirement_content=sanitized_content,
            project_key=request.project_key,
            test_types=request.test_types,
            coverage_level=request.coverage_level
        )
        
        # Ejecutar an√°lisis con LLM
        analysis_result = await llm_wrapper.analyze_jira_workitem(
            prompt=prompt,
            work_item_id=request.work_item_id,
            analysis_id=analysis_id
        )
        
        # Procesar casos de prueba generados
        test_cases = []
        if analysis_result.get("test_cases"):
            for tc_data in analysis_result["test_cases"]:
                test_case = TestCase(
                    test_case_id=tc_data.get("test_case_id", f"TC-{request.project_key}-001"),
                    title=tc_data.get("title", ""),
                    description=tc_data.get("description", ""),
                    test_type=tc_data.get("test_type", "functional"),
                    priority=tc_data.get("priority", "medium"),
                    steps=tc_data.get("steps", []),
                    expected_result=tc_data.get("expected_result", ""),
                    preconditions=tc_data.get("preconditions", []),
                    test_data=tc_data.get("test_data", {}),
                    automation_potential=tc_data.get("automation_potential", "medium"),
                    estimated_duration=tc_data.get("estimated_duration", "5-10 minutes")
                )
                test_cases.append(test_case)
        
        # Calcular tiempo de procesamiento
        processing_time = (datetime.utcnow() - start_time).total_seconds()
        
        # Crear respuesta
        response = JiraWorkItemResponse(
            work_item_id=request.work_item_id,
            jira_data=jira_data,
            analysis_id=analysis_id,
            status="completed",
            test_cases=test_cases,
            coverage_analysis=analysis_result.get("coverage_analysis", {}),
            confidence_score=analysis_result.get("confidence_score", 0.8),
            processing_time=processing_time,
            created_at=start_time
        )
        
        # Registrar en background task para tracking
        background_tasks.add_task(
            log_jira_workitem_analysis_completion,
            analysis_id,
            request.work_item_id,
            response
        )
        
        logger.info(
            "Jira work item analysis completed",
            work_item_id=request.work_item_id,
            analysis_id=analysis_id,
            test_cases_count=len(test_cases),
            processing_time=processing_time
        )
        
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(
            "Jira work item analysis failed",
            work_item_id=request.work_item_id,
            analysis_id=analysis_id,
            error=str(e)
        )
        raise HTTPException(
            status_code=500,
            detail=f"Error analyzing Jira work item: {str(e)}"
        )

@app.post("/generate-istqb-tests", 
          response_model=ISTQBTestGenerationResponse,
          summary="Generar casos de prueba con t√©cnicas ISTQB",
          description="Genera casos de prueba aplicando t√©cnicas de dise√±o ISTQB Foundation Level",
          tags=["ISTQB", "Generaci√≥n Avanzada"],
          responses={
              200: {
                  "description": "Generaci√≥n de casos ISTQB completada exitosamente",
                  "model": ISTQBTestGenerationResponse
              },
              400: {
                  "description": "Datos de entrada inv√°lidos",
                  "content": {
                      "application/json": {
                          "example": {"detail": "Datos de entrada inv√°lidos"}
                      }
                  }
              },
              500: {
                  "description": "Error interno del servidor",
                  "content": {
                      "application/json": {
                          "example": {"detail": "Error interno del servidor"}
                      }
                  }
              }
          })
async def generate_istqb_test_cases(
    request: ISTQBTestGenerationRequest,
    background_tasks: BackgroundTasks
):
    """
    ## Generar Casos de Prueba con T√©cnicas ISTQB
    
    Genera casos de prueba aplicando t√©cnicas de dise√±o ISTQB Foundation Level con observabilidad completa.
    
    ### Proceso:
    1. **An√°lisis de Configuraci√≥n**: Se procesa la configuraci√≥n JSON del sistema
    2. **Aplicaci√≥n de T√©cnicas**: Se aplican las t√©cnicas ISTQB especificadas
    3. **Generaci√≥n Estructurada**: Se crean casos en formato CSV, fichas y artefactos t√©cnicos
    4. **Plan de Ejecuci√≥n**: Se genera plan de ejecuci√≥n automatizado (opcional)
    5. **Observabilidad**: Se registra en Langfuse para monitoreo y an√°lisis
    
    ### T√©cnicas ISTQB Soportadas:
    - **Equivalencia**: Partici√≥n de clases de equivalencia v√°lidas/inv√°lidas
    - **Valores L√≠mite**: Casos min-1, min, min+1, max-1, max, max+1
    - **Tabla de Decisi√≥n**: Matriz de condiciones y acciones
    - **Transici√≥n de Estados**: Estados y transiciones del sistema
    - **√Årbol de Clasificaci√≥n**: Clases y restricciones entre factores
    - **Pairwise**: Combinaciones m√≠nimas que cubren todas las parejas
    - **Casos de Uso**: Flujos principales y alternos
    - **Error Guessing**: Hip√≥tesis de fallos del dominio
    - **Checklist**: Verificaci√≥n gen√©rica de calidad
    
    ### Formato de Salida:
    - **Secci√≥n A**: CSV con casos de prueba (CP - NNN - PROGRAMA - MODULO - CONDICION - ESCENARIO)
    - **Secci√≥n B**: Fichas detalladas con precondiciones y resultados esperados
    - **Secci√≥n C**: Artefactos t√©cnicos seg√∫n t√©cnicas seleccionadas
    - **Secci√≥n D**: Plan de ejecuci√≥n automatizado (opcional)
    
    ### Respuesta:
    - **csv_cases**: Lista de casos en formato CSV
    - **fichas**: Fichas detalladas de cada caso
    - **artefactos_tecnicos**: Artefactos generados por las t√©cnicas
    - **plan_ejecucion**: Plan de ejecuci√≥n automatizado
    - **confidence_score**: Puntuaci√≥n de confianza (0-1)
    - **processing_time**: Tiempo de procesamiento en segundos
    """
    start_time = datetime.utcnow()
    generation_id = f"istqb_{request.programa}_{int(start_time.timestamp())}"
    
    try:
        logger.info(
            "Starting ISTQB test case generation",
            programa=request.programa,
            generation_id=generation_id,
            modulos_count=len(request.modulos),
            tecnicas_count=sum(1 for v in request.tecnicas.values() if v)
        )
        
        # Generar prompt ISTQB
        prompt = prompt_templates.get_istqb_test_generation_prompt(
            programa=request.programa,
            dominio=request.dominio,
            modulos=request.modulos,
            factores=request.factores,
            limites=request.limites,
            reglas=request.reglas,
            tecnicas=request.tecnicas,
            priorizacion=request.priorizacion,
            cantidad_max=request.cantidad_max,
            salida_plan_ejecucion=request.salida_plan_ejecucion
        )
        
        # Ejecutar generaci√≥n con LLM
        generation_result = await llm_wrapper.generate_istqb_test_cases(
            prompt=prompt,
            programa=request.programa,
            generation_id=generation_id
        )
        
        # Calcular tiempo de procesamiento
        processing_time = (datetime.utcnow() - start_time).total_seconds()
        
        # Crear respuesta
        response = ISTQBTestGenerationResponse(
            programa=request.programa,
            generation_id=generation_id,
            status="completed",
            csv_cases=generation_result.get("csv_cases", []),
            fichas=generation_result.get("fichas", []),
            artefactos_tecnicos=generation_result.get("artefactos_tecnicos", {}),
            plan_ejecucion=generation_result.get("plan_ejecucion", {}),
            confidence_score=generation_result.get("confidence_score", 0.8),
            processing_time=processing_time,
            created_at=start_time
        )
        
        # Registrar en background task para tracking
        background_tasks.add_task(
            log_istqb_generation_completion,
            generation_id,
            request.programa,
            response
        )
        
        logger.info(
            "ISTQB test case generation completed",
            programa=request.programa,
            generation_id=generation_id,
            csv_cases_count=len(response.csv_cases),
            fichas_count=len(response.fichas),
            processing_time=processing_time
        )
        
        return response
        
    except Exception as e:
        logger.error(
            "ISTQB test case generation failed",
            programa=request.programa,
            generation_id=generation_id,
            error=str(e)
        )
        raise HTTPException(
            status_code=500,
            detail=f"Error generating ISTQB test cases: {str(e)}"
        )

@app.get("/analysis/{analysis_id}")
async def get_analysis_result(analysis_id: str):
    """
    Obtener resultado de an√°lisis por ID
    """
    try:
        # Aqu√≠ implementar√≠as la l√≥gica para recuperar el an√°lisis
        # Por ahora retornamos un placeholder
        return {
            "analysis_id": analysis_id,
            "status": "completed",
            "message": "Analysis result retrieval not implemented yet"
        }
    except Exception as e:
        logger.error("Failed to get analysis result", analysis_id=analysis_id, error=str(e))
        raise HTTPException(status_code=404, detail="Analysis not found")

async def log_analysis_completion(
    analysis_id: str,
    test_case_id: str,
    response: TestCaseAnalysisResponse
):
    """Background task para registrar la finalizaci√≥n del an√°lisis"""
    try:
        # Aqu√≠ podr√≠as implementar l√≥gica adicional como:
        # - Guardar en base de datos
        # - Enviar notificaciones
        # - Actualizar m√©tricas
        logger.info(
            "Analysis completion logged",
            analysis_id=analysis_id,
            test_case_id=test_case_id
        )
    except Exception as e:
        logger.error(
            "Failed to log analysis completion",
            analysis_id=analysis_id,
            error=str(e)
        )

async def log_requirements_analysis_completion(
    analysis_id: str,
    requirement_id: str,
    response: RequirementsAnalysisResponse
):
    """Background task para registrar la finalizaci√≥n del an√°lisis de requerimientos"""
    try:
        # Aqu√≠ podr√≠as implementar l√≥gica adicional como:
        # - Guardar en base de datos
        # - Enviar notificaciones
        # - Actualizar m√©tricas
        logger.info(
            "Requirements analysis completion logged",
            analysis_id=analysis_id,
            requirement_id=requirement_id,
            test_cases_count=len(response.test_cases)
        )
    except Exception as e:
        logger.error(
            "Failed to log requirements analysis completion",
            analysis_id=analysis_id,
            error=str(e)
        )

async def log_jira_workitem_analysis_completion(
    analysis_id: str,
    work_item_id: str,
    response: JiraWorkItemResponse
):
    """Background task para registrar la finalizaci√≥n del an√°lisis de work item de Jira"""
    try:
        # Aqu√≠ podr√≠as implementar l√≥gica adicional como:
        # - Guardar en base de datos
        # - Enviar notificaciones
        # - Actualizar m√©tricas
        # - Crear casos de prueba en Jira
        logger.info(
            "Jira work item analysis completion logged",
            analysis_id=analysis_id,
            work_item_id=work_item_id,
            test_cases_count=len(response.test_cases)
        )
    except Exception as e:
        logger.error(
            "Failed to log Jira work item analysis completion",
            analysis_id=analysis_id,
            error=str(e)
        )

async def log_istqb_generation_completion(
    generation_id: str,
    programa: str,
    response: ISTQBTestGenerationResponse
):
    """Background task para registrar la finalizaci√≥n de la generaci√≥n ISTQB"""
    try:
        # Aqu√≠ podr√≠as implementar l√≥gica adicional como:
        # - Guardar en base de datos
        # - Enviar notificaciones
        # - Actualizar m√©tricas
        # - Crear casos de prueba en Jira
        # - Generar reportes de cobertura
        logger.info(
            "ISTQB test generation completion logged",
            generation_id=generation_id,
            programa=programa,
            csv_cases_count=len(response.csv_cases),
            fichas_count=len(response.fichas),
            artefactos_count=len(response.artefactos_tecnicos)
        )
    except Exception as e:
        logger.error(
            "Failed to log ISTQB generation completion",
            generation_id=generation_id,
            error=str(e)
        )

if __name__ == "__main__":
    import uvicorn
    
    port = int(os.getenv("PORT", 8000))
    log_level = os.getenv("LOG_LEVEL", "info").lower()
    
    logger.info("Starting Microservicio de An√°lisis QA", port=port, log_level=log_level)
    
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=port,
        reload=True,
        log_level=log_level
    )
